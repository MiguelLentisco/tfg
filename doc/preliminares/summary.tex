% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter*{Summary}\label{ch:summary}
%\addcontentsline{toc}{chapter}{Summar}

The main goal of this project is to tackle research problems where there has been little research on, focusing on the domain of time series and resolving these problems using models based on LSTM neural networks architectures. First, we present a brief background of the project with an introduction to its basis, the motivation that led to its realization, objectives to accomplish and the structure of this document. Then, the first part formed by the basic concepts used in the development of this project: Deep Learning, time series and metrics. The basics of Deep Learning starts with a short tour in the neural networks  history to learn about its origins that lead to the most basic but functional neural network: the feed-forward neural network. We explain in detail how this model works allowing us to understand the majority of neural networks models used nowadays, as these networks share the fundamentals to a large degree of similarity. Finally, we list a few of the most well-known neural network architectures, focusing in detail at the LSTM cells. Regarding the time series, we present the main results of the actual research on the time series analysis with the mathematical foundation. First, we start with the probabilistic theory that includes the stochastic and stationary processes. Then the lineal models like ARMA that we can use with several techniques like the time series decomposition or the time series \emph{differencing}, fusing both approaches into the ARIMA and SARIMA models. We also introduce and explain a few techniques for time series \emph{discretization} with some examples. In the last section of this part, we make a quick review of the metrics that we are going to use to validate the models developed in the project: for classification problems the accuracy, F-score and the precision-recall curves; for regression the usual error metrics like mean squared error or mean absolute error. The first part of this project is about model selection in classification problems that is classically done with the cross-validation and the hold-out-test validation. We study a new heuristic called Perturbation Validation that does not use a train/test partition but introduces tiny perturbations and measures how the model behaves to them. We experiment with several machine learning models, including a LSTM neural network in a big dataset of time series, both with model selection and hyper-parameters selection, and we analyze the results to validate the effectiveness of this heuristic. In the second and last part we address the anomaly detection problem: detecting atypical behavior in time series. We comment about the big obstacles in this type of problems: the lack of datasets with labeled data, for its use in the training of models based on supervised learning; and the models developed are too specific for the problem they are solving thus they can not be deployed in other areas. We provide a simple yet useful model based in LSTM architecture that can detect several types of anomalies, provided a well-known pattern in the data, and several methods to modify the normal series in order to create artificially samples of anomalies that we use to validate our model. We evaluate the detector and the methods using them with both a real and synthetic dataset, analyzing the performance of the detector and the quality of the perturbations.

\paragraph{KEYWORDS:}
\begin{itemize*}[label=,itemsep=1em,itemjoin=\hspace{1em}]
  \item neural networks
  \item LSTM
  \item time series
  \item model selection
  \item validation
  \item hyper-parameters selection
  \item anomaly detection
  \item detector
  \item perturbation
\end{itemize*}

% Al finalizar el resumen en inglés, volvemos a seleccionar el idioma español para el documento
\selectlanguage{spanish}
\endinput
