% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Introducción
%*******************************************************

% \manualmark
% \markboth{\textsc{Introducción}}{\textsc{Introducción}}

\chapter{Introducción}\label{ch:introduccion}

En las dos últimas décadas, gracias al uso de las GPUs \cite{oh2004gpu} se ha hecho posible un uso potente y extendido del \textbf{aprendizaje profundo} (\textit{deep learning}), una de las subáreas del \textbf{aprendizaje automático} (\emph{machine learning}) más conocidas y usadas actualmente debido a su gran versatilidad de aplicación en diversos tipos de problemas, su fácil diseño y su buen rendimiento. Este campo intenta resolver problemas muy diversos que no tienen una solución fácil de diseñar \textbf{manualmente} (reconocimiento en imágenes, conducción autónoma, domótica...), mediante el uso de las \textbf{redes neuronales} (\emph{Neural Networks}) \cite{landahl1943statistical, hebb1949organization}: algoritmos inspirados en el sistema neuronal del cerebro que, como el propio nombre indica, se estructuran mediante capas formadas por \emph{neuronas} que están \emph{conectadas} entre sí formando así una red compleja donde se propaga la información de una capa a otra.

Dentro de las muy variadas y complejas arquitecturas y tipos de capas que se han ido desarrollando destacamos las \textbf{redes neuronales recurrentes} (\emph{recurrent neural networks}, RNN) \cite{hopfield1982neural, jordan1997serial}, redes neuronales que introducen una novedad: la extracción y guardado de información de una entrada en un instante determinado ($x_t$) para su uso posterior ($x_{t+1}$); en otras palabras, la red utiliza no solo la entrada/dato actual sino también tiene en cuenta \textbf{relaciones de las entradas anteriores}, creándose de esta manera una especie de dependencia temporal.

La más conocida dentro de este tipo de arquitecturas es la \emph{Long Short Term Memory} (LSTM) \cite{hochreiter1997long} que intenta solventar un gran problema dentro de las RNN: la pérdida de dependencias temporales \textbf{a largo plazo}. Implementa un tipo especial de \emph{neurona} llamada \textbf{célula de memoria} para solucionar esto, permitiendo aprender u olvidar información del pasado según convenga \cite{wang2017origin}.

En este contexto surge el dominio sobre el que nos centraremos: las \textbf{series temporales}, realizaciones muestrales de \textbf{procesos estocásticos} o dicho de otra manera, sucesiones de datos ordenados por el tiempo. Actualmente, este tipo de datos abundan en una gran cantidad de problemas variados como por ejemplo, predicción de valores (mercado de valores, contagios, texto predictivo), clasificación de series (electrocardiogramas, movimientos) o detección de anomalías (sensores, tráfico servidores) \cite{wang2017origin}.

De esta manera, aprovecharemos la potencia de las redes neuronales LSTM para extraer la información temporal que contienen las series temporales creando modelos que nos permitan obtener un rendimiento y mejor entendimiento de los problemas con series.

\section{Motivación}

En este campo tan extenso, existen muchos problemas que se han tratado poco o que son muy recientes y todavía no se han investigado; de aquí surge nuestra idea de ahondar en estos dominios de problemas de series temporales tan poco explorados. Nos centraremos así en dar soluciones a estos problemas mediante arquitecturas de redes neuronales LSTM para aprovechar la especialización en cuanto a la extracción y uso de información de patrones y dependencias temporales que generalmente existen en las series.

\section{Objetivos}

Los objetivos fundamentales se centrarán en el estudio, investigación y obtención de resultados en problemas pocos tratados actualmente mediante el uso de técnicas basadas en \emph{Deep Learning}, concretamente modelos de redes neuronales con arquitecturas que utilizan capas LSTM.

Trataremos dos problemas diferentes, cada uno desarrollados en una parte independiente:

\begin{enumerate}
  \item Selección de modelos para clasificación de series temporales (\autoref{part:pv}).
  \item Detección de anomalías en series temporales (\autoref{part:ad}).
\end{enumerate}

\section{Estructura}

Este trabajo se estructura fundamentalmente en tres partes, cada una de las cuales se organiza en varios capítulos. Además se incluye la introducción (\autoref{ch:introduccion}) en la que se presentan el contexto, motivación, objetivos y estructura de este trabajo.

En la primera parte (\autoref{part:conceptos-previos}) desarrollamos un marco teórico de los conceptos necesarios para poder entender este trabajo: sobre el aprendizaje profundo (\autoref{ch:dl}), series temporales (\autoref{ch:st}) y las métricas usadas para valorar nuestros modelos (\autoref{ch:metricas}).

En la segunda parte (\autoref{part:pv}) abordamos el problema de selección de modelos para clasificación de series temporales con una pequeña introducción contextual (\autoref{ch:pv-introduccion}), la descripción y desarrollo del problema de selección junto con la nueva heurística que estudiaremos (\autoref{ch:pv-seleccion}) y toda la experimentación realizada (\autoref{ch:pv-experimentacion}).

En la tercera y última parte (\autoref{part:ad}) nos centramos en el problema de la detección de anomalías en series temporales con una breve introducción al problema (\autoref{ch:ad-introduccion}), el detector que implementaremos detallado (\autoref{ch:ad-detector}), las diferentes alteraciones para crear series anómalas (\autoref{ch:ad-alteraciones}) y los distintos experimentos desarrollados (\autoref{ch:ad-experimentacion}).

Finalmente un breve capítulo (\autoref{ch:conclusiones-trabajo}) sobre las conclusiones obtenidas en las investigaciones de cada parte y el trabajo futuro de posibles investigaciones derivado de este proyecto.

Se han incluido además dos apéndices con la documentación más relevante de las clases y funciones implementadas (\autoref{ap:documentacion}) y otro con información sobre el software desarrollado (\autoref{ap:software}).

\endinput
